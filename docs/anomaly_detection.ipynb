{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import scipy.stats as st\n",
    "from scipy.special import binom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from context import predicate_search\n",
    "from predicate_search import NormData, PredicateAnomalyDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y, predy):\n",
    "    true_positives = true_positives = ((y == 1).astype(int) * (predy == 1).astype(int)).sum()\n",
    "    if predy.sum() == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return true_positives / predy.sum()\n",
    "\n",
    "def recall(y, predy):\n",
    "    true_positives = ((y == 1).astype(int) * (predy == 1).astype(int)).sum()\n",
    "    if y.sum() == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return true_positives / y.sum()\n",
    "\n",
    "def f1(y, predy):\n",
    "    p = precision(y, predy)\n",
    "    r = recall(y, predy)\n",
    "    if p + r == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2 * (p*r) / (p + r)\n",
    "\n",
    "def accuracy(y, predy):\n",
    "    return ((predy == y).astype(int).mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(object):\n",
    "    \n",
    "    def __init__(self, model, params, eval_funcs=[precision, recall, f1, accuracy]):\n",
    "        self.model = model\n",
    "        self.params = params\n",
    "        self.eval_funcs = eval_funcs\n",
    "        self.all_params = [dict(zip(self.params.keys(), v)) for v in itertools.product(*self.params.values())]\n",
    "        \n",
    "    def score_param(self, X, y, param_dict):\n",
    "        m = self.model(**param_dict)\n",
    "        predy = m.fit_predict(X)\n",
    "        for f in self.eval_funcs:\n",
    "            param_dict[f.__name__] = f(y, predy)\n",
    "        param_dict['model'] = m\n",
    "        return param_dict\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return pd.DataFrame([self.score_param(X, y, param_dict) for param_dict in self.all_params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNormal:\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        \n",
    "    def test(self, param_dict, models, model_params, n_samples):\n",
    "        all_res = []\n",
    "        for i in range(n_samples):\n",
    "            norm_data = NormData(**param_dict)\n",
    "            for j in range(len(models)):\n",
    "                model = models[j]\n",
    "                params = model_params[j]\n",
    "                test = TestModel(model, params)\n",
    "                res = test.score(norm_data.tainted, norm_data.y)\n",
    "                res['data'] = norm_data\n",
    "                all_res.append(res)\n",
    "        return all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# norm_params = {'n': [1000], 'm': [2]}\n",
    "# model_params = {'c': np.linspace(.1, 1, 10), 'b': [0, .1, .2]}\n",
    "# test_normal = TestNormal(norm_params)\n",
    "# res = pd.concat(test_normal.test({'n': 1000, 'm': 2}, [PredicateAnomalyDetection], [model_params], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = pd.concat(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_data = NormData(1000, 2, k=2)\n",
    "# data = norm_data.tainted\n",
    "# y = norm_data.y\n",
    "# norm_data.predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_data.plot2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = PredicateAnomalyDetection()\n",
    "# m.fit(data)\n",
    "# m.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'c': np.linspace(.1, 1, 10), 'b': [0, .1, .2]}\n",
    "# test = TestModel(PredicateAnomalyDetection, params)\n",
    "# res = test.score(data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:advent_dev] *",
   "language": "python",
   "name": "conda-env-advent_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
